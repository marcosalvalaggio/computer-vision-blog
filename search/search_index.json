{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A brief introduction","text":"<p>The purpose of the blog is to showcase the computer vision theories I have studied. I aim to publish a new post each month, covering a different topic and providing a detailed explanation.</p> <p>Every section/post is produced using a markdown file obtained by converting a jupyter notebook. The main frameworks used for the studies are:</p> <ul> <li>OpenCV</li> <li>NumPy</li> <li>Matplotlib</li> </ul>"},{"location":"GeometricTransformations/","title":"Geometric transformations","text":""},{"location":"GeometricTransformations/#geometric-transformations_1","title":"Geometric transformations","text":"<p>A geometric transform (image rotations or general warps) is a modification of the spatial relationship among pixels. </p> <p> Basic set of 2D geometric image transformations (planar transformations). <p></p> <p>Image warping involves modifying the domain of an image function. We can represent the operation for obtaining the transformed image with two steps:</p> <ul> <li>Coordinate trasform: \\((x',y')=T\\{(x,y)\\}\\)</li> <li>Image mapping/resampling </li> </ul>"},{"location":"GeometricTransformations/#coordinate-trasform","title":"Coordinate trasform:","text":"<p>Points in 2D can be expressed in homogeneous coordinates. In this way, we can represent transformations more compactly only with matrix multiplication. From 2D point to homogeneous coordinates:</p> \\[\\begin{equation} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} \\tilde{w}x \\\\ \\tilde{w}y \\\\ \\tilde{w} \\end{bmatrix} = \\begin{bmatrix} \\tilde{x} \\\\ \\tilde{y} \\\\ \\tilde{w} \\end{bmatrix}, \\end{equation}\\] <p>From homogeneous coordinates to 2D point: </p> \\[\\begin{equation} \\begin{bmatrix} \\tilde{x} \\\\ \\tilde{y} \\\\ \\tilde{w} \\end{bmatrix} = \\begin{bmatrix} \\tilde{x}/\\tilde{w} \\\\ \\tilde{y}/\\tilde{w} \\end{bmatrix}. \\end{equation}\\]"},{"location":"GeometricTransformations/#translation-in-homogeneous-coords","title":"Translation in homogeneous coords:","text":"<ul> <li>Translation: </li> </ul> \\[\\begin{equation} \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} +  \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}, \\end{equation}\\] <ul> <li>Translation in hom coords: </li> </ul> \\[\\begin{equation} \\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; b_1 \\\\ 0 &amp; 1 &amp; b_2 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}, \\end{equation}\\] <ul> <li>Yieldng: </li> </ul> \\[\\begin{equation} \\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; b_1 \\\\ 0 &amp; 1 &amp; b_2 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x+b_1\\\\ y+b_2\\\\ 1 \\end{bmatrix} \\end{equation}\\]"},{"location":"GeometricTransformations/#affine-transform","title":"Affine transform","text":"<p>Is a geometric transformation that preserves point collinearity and distance ratios along a line. Affine transform can be interpreted as a linear transform followed by a translation. </p> \\[ \\begin{equation} \\begin{bmatrix} x' \\\\ y' \\end{bmatrix} =A \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}, \\end{equation}\\] <p>Homogeneous coordinates: multiple operations combined into a single matrix multiplication</p> \\[\\begin{equation} \\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix} = T\\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix}  =  \\begin{bmatrix} t_{1,1} &amp; t_{1,2} &amp; t_{1,3} \\\\ t_{2,1} &amp; t_{2,2} &amp; t_{2,3} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}, \\end{equation}\\] <p>\\([t_{1,1},t_{1,2},t_{2,1},t_{2,2}]\\) represents the effect of the linear transformation (A), and \\([t_{1,3},t_{2,3}]\\) represents the effect of the traslation applied with vector \\(\\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}\\)  .</p>"},{"location":"GeometricTransformations/#examples-of-trasformations-with-homogeneous-coordinates","title":"Examples of trasformations with homogeneous coordinates:","text":"<p> <p></p>"},{"location":"GeometricTransformations/#image-mappingresampling","title":"Image mapping/resampling","text":"<p> <p></p>"},{"location":"GeometricTransformations/#opencv-transformations","title":"OpenCV Transformations","text":"<p>OpenCV provides two transformation functions, cv2.warpAffine and cv2.warpPerspective, with which you can perform all kinds of transformations. cv2.warpAffine takes a 2x3 transformation matrix while cv2.warpPerspective takes a 3x3 transformation matrix as input.</p> <pre><code># Libraries\nimport cv2 \nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline\n</code></pre>"},{"location":"GeometricTransformations/#scaling","title":"Scaling","text":"<p>Scaling is just resizing of the image. OpenCV comes with a function cv2.resize for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are cv2.INTER_AREA for shrinking and cv2.INTER_CUBIC (slow) &amp; cv2.INTER_LINEAR for zooming. By default, the interpolation method cv2.INTER_LINEAR is used for all resizing purposes. You can resize an input image with either methods. The scaling equation are represented in matrix form in homogeneus coordinates as:</p> \\[\\begin{equation} \\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix} = T\\begin{bmatrix} x' \\\\ y'  \\\\ 1 \\end{bmatrix}  =  \\begin{bmatrix} f_x &amp; 0 &amp; 0 \\\\ 0 &amp; f_y &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}, \\end{equation}\\] <p>The complete syntax for cv2.resize is given below:</p> <p><code>cv2.resize(src, dst, Size(), 0.5, 0.5, interpolation)</code></p> <ul> <li>src: input image.</li> <li>dst: output image; it has the size dsize (when it is non-zero) or the size computed from src.size(), fx, and fy; the type of dst is the same as of src.</li> <li>dsize: output image size; if it equals to None, it is computed as: \\(dsize = Size(round(f_x\\cdot src.cols(width)), round(f_y\\cdot src.rows(height)))\\) Either dsize or both fx and fy must be non-zero.</li> <li>\\(f_x\\): scale factor along the horizontal axis</li> <li>\\(f_y\\): scale factor along the vertical axis</li> <li>interpolation: interpolation method</li> </ul> <pre><code>img = cv2.imread('ditto.jpeg',1)\nprint(f'Dimensions of image: {img.shape}') # (height, width)\nheight, width = img.shape[:2]\n</code></pre> <pre><code>Dimensions of image: (430, 510, 3)\n</code></pre> <pre><code># double the dimensions of the image \nres = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n# new dimensions \nprint(f'Dimensions of resized image: {res.shape}')\n# plot \nres_rgb = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\nplt.imshow(res_rgb)\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>Dimensions of resized image: (860, 1020, 3)\n</code></pre> <p> <p></p> <pre><code># alternative version (half the original dimensions)\nres_2 = cv2.resize(img,None,fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\nprint(f'Dimensions of resized image: {res_2.shape}')\n# None represent the output image\n</code></pre> <pre><code>Dimensions of resized image: (215, 255, 3)\n</code></pre> <pre><code># simple version (specific dimensions for the resized image)\nimg = cv2.imread('ditto.jpeg',1)\nprint(img.shape)\nimg = cv2.resize(img, (600,500)) #(width,height)\nprint(img.shape)\n</code></pre> <pre><code>(430, 510, 3)\n(500, 600, 3)\n</code></pre>"},{"location":"GeometricTransformations/#translation","title":"Translation","text":"<p>Translation is the shifting of an object's location. If you know the shift in the (x,y) direction and let it be (\\(t_x\\),\\(t_y\\)), you can create the transformation matrix M (reduction of T matrix) as follows:</p> \\[M=\\begin{bmatrix} 1 &amp; 0 &amp; t_x \\\\ 0 &amp; 1 &amp; t_y \\end{bmatrix}\\] <ul> <li>Providing positive values for \\(t_x\\) will shift the image to right and negative values will shift the image to the left.</li> <li>Similarly, positive values of \\(t_y\\) will shift the image down while negative values will shift the image up.</li> </ul> <p>You can make it into a Numpy array of type np.float32 and pass it into the cv.warpAffine() function.</p> <pre><code>img = cv2.imread('ditto.jpeg',1)\nrows,cols = img.shape[:2]\n# tx and ty values for translation\ntx = 100\nty = 100\nM = np.array([[1, 0, tx],[0, 1, ty]], dtype=np.float32)\n# apply the translation to the image\ndst = cv2.warpAffine(img,M,(cols,rows))\n# plot \nprint(f'Dimensions of traslated image: {dst.shape}')\ndst_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\nplt.imshow(dst_rgb)\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>Dimensions of traslated image: (430, 510, 3)\n</code></pre> <p> <p></p> <p>The third argument of the cv2.warpAffine function is the size of the output image, which should be in the form of (width, height). Remember width = number of columns, and height = number of rows.</p>"},{"location":"GeometricTransformations/#rotation","title":"Rotation","text":"<p>Rotation of an image for an angle \\(\\theta\\) is achieved by the transformation matrix of the form:</p> \\[M=\\begin{bmatrix} cos\\theta &amp; -sin\\theta \\\\ sin\\theta &amp; cos\\theta \\end{bmatrix}\\] <p>OpenCV provides the ability to define the center of rotation for the image and a scale factor to resize the image as well. In that case, the transformation matrix gets modified.</p> \\[\\begin{bmatrix} \\alpha &amp; \\beta &amp; (1-\\alpha)\\cdot c_x-\\beta\\cdot c_y \\\\ -\\beta &amp; \\alpha &amp; \\beta\\cdot c_x+(1-\\alpha)\\cdot c_y \\end{bmatrix}\\] <p>In the above matrix:</p> <ul> <li>\\(\\alpha = scale \\cdot cos\\theta\\)</li> <li>\\(\\beta = scale \\cdot sin\\theta\\)</li> </ul> <p>where \\(c_x\\) and \\(c_y\\) are the coordinates along which the image is rotated. OpenCV provides the getRotationMatrix2D function to create the above transformation matrix. The following is the syntax for creating the 2D rotation matrix:</p> <p><code>getRotationMatrix2D(center, angle, scale)</code></p> <ul> <li>center: the center of rotation for the input image,</li> <li>angle: the angle of rotation in degrees,</li> <li>scale: an isotropic scale factor which scales the image up or down according to the value provided, this can be a floating point value. For example, a value of 1.0 will keep the output image the same size as the source image. And a value of 2.0 will make the resulting image double the size of the source image</li> </ul> <p>The function returns the 2D-rotation matrix, which will be used in the next step to rotate the image. If the angle is positive, the image gets rotated in the counter-clockwise direction. If you want to rotate the image clockwise by the same amount, then the angle needs to be negative.</p>"},{"location":"GeometricTransformations/#rotation-is-a-three-step-operation","title":"Rotation is a three-step operation:","text":"<ul> <li>First, you need to get the center of rotation. This typically is the center of the image you are trying to rotate.</li> <li>Next, create the 2D-rotation matrix. OpenCV provides the cv2.getRotationMatrix2D function that we discussed above. </li> <li>Finally, apply the affine transformation to the image, using the rotation matrix you created in the previous step. The cv2.warpAffine function in OpenCV does the job.</li> </ul> <p>The cv2.warpAffine function applies an affine transformation to the image. After applying affine transformation, all the parallel lines in the original image will remain parallel in the output image as well. The complete syntax for cv2.warpAffine is given below:</p> <p><code>warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])</code></p> <ul> <li>src: the source mage</li> <li>M: the transformation matrix</li> <li>dsize: size of the output image</li> <li>dst: the output image</li> <li>flags: combination of interpolation methods such as INTER_LINEAR or INTER_NEAREST</li> <li>borderMode: the pixel extrapolation method</li> <li>borderValue: the value to be used in case of a constant border, has a default value of 0</li> </ul> <pre><code>img = cv2.imread('ditto.jpeg',1)\n# Dividing height and width by 2 to get the center of the image\nheight, width = img.shape[:2]\ncenter = (width/2, height/2)\nM = cv2.getRotationMatrix2D(center=center, angle=45, scale=1)\n# Rotate the image using cv2.warpAffine\nrtd = cv2.warpAffine(src=img, M=M, dsize=(width, height))\n# plot \nprint(f'Dimensions of rotated image: {rtd_rgb.shape}')\nrtd_rgb = cv2.cvtColor(rtd, cv2.COLOR_BGR2RGB)\nplt.imshow(rtd_rgb)\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>Dimensions of rotated image: (430, 510, 3)\n</code></pre> <p> <p></p>"},{"location":"GeometricTransformations/#affine-transformation","title":"Affine Transformation","text":"<ul> <li>A transformation that can be expressed in the form of a matrix multiplication (linear transformation) followed by a vector addition (translation).</li> <li> <p>From the above, we can use an Affine Transformation to express:</p> <ul> <li>Rotations (linear transformation)</li> <li>Translations (vector addition)</li> <li>Scale operations (linear transformation)  </li> <li>In essence, an Affine Transformation represents a relation between two images.</li> <li>The usual way to represent an Affine Transformation is by using a 2\u00d73 matrix. </li> </ul> </li> </ul> \\[A = \\begin{bmatrix} a_{0,0} &amp; a_{0,1} \\\\ a_{1,0} &amp; a_{1,1}\\end{bmatrix}, \\;\\; B=\\begin{bmatrix} b_{0,0} \\\\ b_{1,0}\\end{bmatrix}\\] \\[ M = [A \\;B] =  \\begin{bmatrix} a_{0,0} &amp; a_{0,1} &amp; b_{0,0} \\\\ a_{1,0} &amp; a_{1,1} &amp; b_{1,0}\\end{bmatrix}\\] <p>Considering that we want to transform a 2D vector \\(X = \\begin{bmatrix} x \\\\ y \\end{bmatrix}\\), by using A and B, we can do the same with:</p> \\[T=A \\begin{bmatrix} x \\\\ y \\end{bmatrix} + B = M\\cdot[x,y,1]^T\\] \\[ T= \\begin{bmatrix} a_{0,0}x + a_{0,1}y + b_{0,0} \\\\ a_{1,0}x + a_{1,1}y + b_{1,0}\\end{bmatrix}\\] <p>In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need three points from the input image and their corresponding locations in the output image. Then cv2.getAffineTransform will create a 2x3 matrix which is to be passed to cv2.warpAffine.</p> <pre><code>img = cv2.imread('ditto.jpeg',1)\nrows,cols,ch = img.shape\n#  three points and their corresponding locations in the output image\npts1 = np.float32([[50,50],[200,50],[50,200]])\npts2 = np.float32([[10,100],[150,50],[150,250]])\nM = cv2.getAffineTransform(pts1,pts2)\ndst = cv2.warpAffine(img,M,(cols,rows))\n# plot \nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ndst_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\nplt.subplot(121),plt.imshow(img_rgb),plt.title('Input')\nplt.axis('off')\nplt.subplot(122),plt.imshow(dst_rgb),plt.title('Output')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"GeometricTransformations/#rotating-the-image-after-warp","title":"Rotating the image after Warp","text":"<pre><code>rows,cols,ch = img.shape\n#  three points and their corresponding locations in the output image\npts1 = np.array( [[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]] ).astype(np.float32)\npts2 = np.array( [[0, img.shape[1]*0.33], [img.shape[1]*0.85, img.shape[0]*0.25], [img.shape[1]*0.15, img.shape[0]*0.7]] ).astype(np.float32)\nM = cv2.getAffineTransform(pts1,pts2)\ndst = cv2.warpAffine(img,M,(cols,rows))\n# rotating \ncenter = (dst.shape[1]//2, dst.shape[0]//2) # (width, height)\nangle = -50\nscale = 0.6\nrot_mat = cv2.getRotationMatrix2D( center, angle, scale )\ndst_rot = cv2.warpAffine(dst, rot_mat, (dst.shape[1], dst.shape[0])) # (width, height)\n# plot \nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ndst_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\ndst_rot_rgb = cv2.cvtColor(dst_rot, cv2.COLOR_BGR2RGB)\nplt.subplot(131),plt.imshow(img_rgb),plt.title('Input')\nplt.axis('off')\nplt.subplot(132),plt.imshow(dst_rgb),plt.title('Output')\nplt.axis('off')\nplt.subplot(133),plt.imshow(dst_rot_rgb),plt.title('Output')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"GeometricTransformations/#perspective-transformation","title":"Perspective Transformation","text":"<p>For perspective transformation, you need a 3x3 transformation matrix. Straight lines will remain straight even after the transformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then the transformation matrix can be found by the function cv2.getPerspectiveTransform. Then apply cv2.warpPerspective with this 3x3 transformation matrix.</p> <pre><code>img = cv2.imread('ditto.jpeg',1)\nrows,cols,ch = img.shape\npts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\npts2 = np.float32([[0,0],[400,0],[0,400],[400,400]])\nM = cv2.getPerspectiveTransform(pts1,pts2)\ndst = cv2.warpPerspective(img,M,(300,300))\n# plot \nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ndst_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\nplt.subplot(121),plt.imshow(img_rgb),plt.title('Input')\nplt.axis('off')\nplt.subplot(122),plt.imshow(dst_rgb),plt.title('Output')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"GeometricTransformations/#references","title":"References","text":"<ul> <li>image processing and computer vision by Rein van den Boomgaard</li> <li>Geometric Transformations of Images by OpenCV doc.</li> <li>image rotation and translation using opencv</li> <li>Computer Vision: Algorithms and Applications by Richard Szeliski</li> <li>Computer Vision Course by Prof. Ghidoni Stefano (Unipd)</li> </ul>"},{"location":"HelloWorld/","title":"OpenCV HelloWorld","text":""},{"location":"HelloWorld/#opencv-helloworld_1","title":"OpenCV HelloWorld","text":"<p>OpenCV (Open Source Computer Vision Library: opencv.org) is an open-source library that includes several hundreds of computer vision algorithms.</p> <pre><code>import cv2\nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline\n</code></pre> <p>Reading, displaying, and writing images are basic to image processing and computer vision.</p> <ul> <li>imread() - helps us read an image</li> <li>imshow() - displays an image in a window</li> <li>imwrite() - writes an image into the file directory</li> </ul> <pre><code># The function cv2.imread() is used to read an image.\nimg_bgr = cv2.imread('ditto.jpeg',1)\n# image features\nprint(img_bgr.shape)\nprint(type(img_bgr))\n</code></pre> <pre><code>(430, 510, 3)\n&lt;class 'numpy.ndarray'&gt;\n</code></pre> <p>Beware that cv2.imread() returns a numpy array in BGR not RGB</p>"},{"location":"HelloWorld/#plot-a-color-image","title":"Plot a color image","text":"<p> We plot the image directly into the notebook with matplotlib for simplicity without using the imshow function. Remember that OpenCV upload an image with BGR codification of colour, so if you want to plot with matplotlib you have to convert it into the RGB codification The method imread, imwrite and imshow indeed all work with the BGR order, so there is no need to change the order when you read an image with cv2.imread and then want to show it with cv2.imshow. Finally, BGR and RGB are not color spaces, they are just conventions for the order of the different color channels. </p> <pre><code>img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.show()\n</code></pre> <p> </p>"},{"location":"HelloWorld/#more-details-on-the-imread-function","title":"More details on the imread function","text":"<p> The first argument is the image name, which requires a fully qualified pathname to the file. The second argument is an optional flag that lets you specify how the image should be represented. OpenCV offers several options for this flag, but those that are most common include: </p> <ul> <li>cv2.IMREAD_UNCHANGED  or -1</li> <li>cv2.IMREAD_GRAYSCALE  or 0</li> <li>cv2.IMREAD_COLOR  or 1</li> </ul> <pre><code># The function cv2.imread() is used to read an image.\nimg_grayscale = cv2.imread('ditto.jpeg',0)\n# plot the image \nplt.imshow(img_grayscale, cmap='gray')\nplt.axis('off')\nplt.show()\n</code></pre> <p> </p>"},{"location":"HelloWorld/#save-the-grayscale-image","title":"Save the grayscale image","text":"<pre><code>cv2.imwrite('ditto_gray.jpeg', img_grayscale)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"HelloWorld/#bgr-codification","title":"BGR codification","text":"<p> In OpenCV, an image is composed of pixels that are placed in a tensor. Each pixel contains a colour value as a BGR triplet. For example, a 512x512 image is composed of 512 columns and 512 rows matrix for every color channel, and the total number of pixels present in the image is equal to the number of rows multiplied by the number of columns. </p> <p> A BGR triplet value contains three values that range from 0 to 255 in an 8-bit image. Where 0 means black and 255 means blue in the first matrix of the tensor (the blue channel) or means green or red in the other channels.  </p> <p> In the following examples, I always converted the generated images into RGB codification for simulating the imread process. </p>"},{"location":"HelloWorld/#representing-a-simple-bgr-image","title":"Representing a simple BGR image","text":"<p>Generating a \\(2\\times2\\) colours image with the BGR codification </p> <pre><code># generating a random images \nimg_test = np.random.randint(0,255,(2,2,3), np.uint8)\nprint(f'image shape: {img_test.shape}')\n# blue channel\nimg_blue = img_test.copy()\nimg_blue[:,:,1] = 0\nimg_blue[:,:,2] = 0\nimg_blue = cv2.cvtColor(img_blue, cv2.COLOR_BGR2RGB)\n# green channel\nimg_green = img_test.copy()\nimg_green[:,:,0] = 0\nimg_green[:,:,2] = 0\nimg_green = cv2.cvtColor(img_green, cv2.COLOR_BGR2RGB)\n# red channel \nimg_red = img_test.copy()\nimg_red[:,:,0] = 0\nimg_red[:,:,1] = 0\nimg_red = cv2.cvtColor(img_red, cv2.COLOR_BGR2RGB)\n# generated image \nimg_test = cv2.cvtColor(img_test, cv2.COLOR_BGR2RGB)\n# plot \nplt.figure(figsize=(6,6))\nplt.subplot(2,2,1)\nplt.imshow(img_blue)\nplt.title('blue channel')\nplt.axis('off')\nfor i in range(2):\nfor j in range(2):\n# RGB formatting\nc = img_blue[j,i,2] #values of the blue channel matrix \nplt.text(i, j, str(c), va='center', ha='center', color = 'w')\nplt.subplot(2,2,2)\nplt.imshow(img_green)\nplt.title('green channel')\nplt.axis('off')\nfor i in range(2):\nfor j in range(2):\n# RGB formatting\nc = img_green[j,i,1] #values of the green channel matrix \nplt.text(i, j, str(c), va='center', ha='center', color = 'w')\nplt.subplot(2,2,3)\nplt.imshow(img_red)\nplt.title('red channel')\nplt.axis('off')\nfor i in range(2):\nfor j in range(2):\n# RGB formatting\nc = img_red[j,i,0] #values of the red channel matrix \nplt.text(i, j, str(c), va='center', ha='center', color = 'w')\nplt.subplot(2,2,4)\nplt.imshow(img_test)\nplt.title('generated image')\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>image shape: (2, 2, 3)\n</code></pre> <p> </p>"},{"location":"HelloWorld/#generating-a-black-image","title":"Generating a black image","text":"<pre><code># with NumPy, we can easily create a sample image to study the relation of the BGR codification \nheight = 512\nwidth = 512\nimg_black = np.zeros((height,width,3), np.uint8)\n# image generated\nimg_black = cv2.cvtColor(img_black, cv2.COLOR_BGR2RGB)\nplt.imshow(img_black)\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"HelloWorld/#generating-colours-image","title":"Generating colours image","text":"<pre><code># blue image\nimg_blue = np.zeros((height,width,3), np.uint8)\nimg_blue[:,:] = (255,0,0)\nimg_blue = cv2.cvtColor(img_blue, cv2.COLOR_BGR2RGB)\n# green image \nimg_green = np.zeros((height,width,3), np.uint8)\nimg_green[:,:] = (0,255,0)\nimg_green = cv2.cvtColor(img_green, cv2.COLOR_BGR2RGB)\n# red image \nimg_red = np.zeros((height,width,3), np.uint8)\nimg_red[:,:] = (0,0,255)\nimg_red = cv2.cvtColor(img_red, cv2.COLOR_BGR2RGB)\n# random colours image\nimg_random = np.random.randint(0,255,(height,width,3), np.uint8)\nimg_random = cv2.cvtColor(img_random, cv2.COLOR_BGR2RGB)\n# plot images \nplt.figure(figsize=(6,6))\nplt.subplot(2,2,1)\nplt.imshow(img_blue)\nplt.axis('off')\nplt.subplot(2,2,2)\nplt.imshow(img_green)\nplt.axis('off')\nplt.subplot(2,2,3)\nplt.imshow(img_red)\nplt.axis('off')\nplt.subplot(2,2,4)\nplt.imshow(img_random)\nplt.axis('off')\nplt.show()\n</code></pre>"},{"location":"HelloWorld/#why-did-opencv-choose-bgr-color-space","title":"Why did OpenCV choose BGR color space?","text":"<p> The early developers at OpenCV chose the BGR colour format because back then BGR colour format was popular among camera manufacturers and software providers. BGR was a choice made for historical reasons and now we have to live with it. </p>"},{"location":"LocalOperations/","title":"Local Operations","text":""},{"location":"LocalOperations/#local-operations_1","title":"Local Operations","text":"<p>Local/neighborhood operators can be used to filter images in order to add soft blur, sharpen details, accentuate edges, or remove noise. Local operations are defined based on a filter/kernel. The kernel defines:</p> <ul> <li>A neighborhood (the set of \"green\" pixels)</li> <li>A weight associated with each pixel involved in the computation</li> </ul> <p> <p>Local operations are performed in the spatial domain of the image (the space containing the pixels)</p> <ul> <li>AKA spatial filtering</li> <li>The kernel is AKA spatial filter</li> </ul>"},{"location":"LocalOperations/#type-of-filters","title":"Type of filters","text":"<p>Depending on the processing applied to the image the filter can be:</p> <ul> <li>Linear </li> <li>Non-linear</li> </ul> <p>"},{"location":"LocalOperations/#linear-filters-convolutioncorrelation","title":"Linear Filters (convolution/correlation)","text":"<p>How is the spatial filter applied to the image? output pixel\u2019s value is determined as a weighted sum of input pixel values</p>"},{"location":"LocalOperations/#correlation-operation","title":"Correlation operation","text":"<ul> <li>Filter superimposed on each pixel location</li> <li>Evaluation of a weighted average<ul> <li>Pixel value</li> <li>Filter weight</li> </ul> </li> </ul> <p> <p>Suppose the filter dimensions are \\(m \\times n\\), where:</p> <ul> <li>\\(m = 2a +1\\)</li> <li>\\(n = 2b + 1\\)</li> </ul> <p>Correlation is defined as:</p> \\[g(x,y)= \\sum_{s=-a}^a\\sum_{t=-b}^b w(s,t)I(x+s,y+t)\\] <p>The entries in the weight kernel or mask \\(w(s,t)\\) are often called the filter coefficients. The above correlation operator can be more compactly notated as:</p> \\[g=I\\otimes w\\] <p>A common variant on this formula is:</p> \\[g(x,y)= \\sum_{s=-a}^a\\sum_{t=-b}^b w(s,t)I(x-s,y-t)\\] <p>where the sign of the offsets in \\(I\\) has been reversed. This is called the convolution operator,</p> \\[g = I * w\\] <p>and \\(w\\) is then called the impulse response function. The continuous version of convolution can be written as</p> \\[(I*g)(t) = \\int_{-\\infty}^{\\infty} I(\\tau)g(t-\\tau)d\\tau\\] <p>Both correlation and convolution are linear shift-invariant (LSI) operators, which means that he operator \u201cbehaves the same everywhere\u201d. In the computer vision context, convolution and correlation are often used as synonims. Usually, correlation is evaluated but it is called convolution!</p> <ul> <li>Filters are usually simmetric</li> <li>Filters obtained by applying convolution are called convolutional filters</li> <li>On a grayscale image the filter weights can change the image brightness</li> <li>Brightness is unchanged if: \\(\\sum_i w_i = 1\\)</li> </ul>"},{"location":"LocalOperations/#correlation-vs-convolution-effects","title":"Correlation vs Convolution effects","text":"<pre><code>import cv2 \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom scipy.ndimage import convolve\n%matplotlib inline \n</code></pre> <p>OpenCV provides you with the function filter2D. In a nutshell, with this function, we can convolve (correlate) an image with the kernel (typically a 2d matrix) to apply a filter on the images. The following is the syntax for performing the filter operation:</p> <p><code>filter2D (src, dst, ddepth, kernel, anchor = cv2.Point(-1, -1), delta = 0, borderType = cv2.BORDER_DEFAULT)</code></p> <p>where:</p> <ul> <li>src \u2013 The source image to apply the filter on</li> <li>dst \u2013 Name of the output image after applying the filter</li> <li>ddepth \u2013 Depth of the output image [ -1 will give the output image depth as same as the input image]</li> <li>kernel \u2013 The 2d matrix we want the image to convolve (correlate) with</li> <li>anchor - anchor of the kernel that indicates the relative position of a filtered point within the kernel; the anchor should lie within the kernel; default value new cv2.Point(-1, -1) means that the anchor is at the kernel center</li> <li>delta - optional value added to the filtered pixels before storing them in dst</li> <li>borderType - pixel extrapolation method<ul> <li>BORDER_CONSTANT</li> <li>BORDER_DEFAULT</li> <li>BORDER_REFLECT</li> <li>BORDER_REPLICATE </li> <li>BORDER_WRAP</li> <li>BORDER_ISOLATED</li> </ul> </li> </ul> <pre><code>img = cv2.imread('ditto_gray.jpeg',0)\n# plot original \nplt.figure(figsize=(10,12))\nplt.subplot(131)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image')\n# prepare filter\nkernel = np.array([[1,1,1],[1,1,0],[1,0,0]], dtype = np.float32)\n#kernel = np.ones((5,5), dtype=np.float32)\n#kernel = kernel/9\nprint('\\n')\nprint('Kernel applied:')\nprint(kernel)\nprint('\\n')\n# ----------- #\n# correlation \n# ----------- #\nddepth = -1\nimg_corr = cv2.filter2D(img, ddepth, kernel, borderType = cv2.BORDER_DEFAULT)\nplt.subplot(132)\nplt.imshow(img_corr, cmap = 'gray')\nplt.axis('off')\nplt.title('Correlation filtering')\n# ----------- #\n# convolution (Setting cval=1.0 is equivalent to padding the outer edge of input with 1.0\u2019s (and then extracting only the original region of the result).)\n# ----------- #\nimg_conv  = convolve(img, kernel, mode='constant', cval=1.0)\nplt.subplot(133)\nplt.imshow(img_conv, cmap = 'gray')\nplt.axis('off')\nplt.title('Convolution filtering')\nplt.show()\n</code></pre> <pre><code>Kernel applied:\n[[1. 1. 1.]\n [1. 1. 0.]\n [1. 0. 0.]]\n</code></pre> <p>"},{"location":"LocalOperations/#smoothing","title":"Smoothing","text":""},{"location":"LocalOperations/#identity-filter","title":"Identity filter","text":"<p>Recap: a convolution (correlation) kernel is a 2D matrix that is used to filter images. Also known as a convolution matrix, a convolution (correlation) kernel is typically a square, MxM matrix, where M is an odd integers (e.g. 3\u00d73, 5\u00d75, 7\u00d77, \\(\\dots\\)).</p> <p>Before we describe how to implement blurring and sharpening kernels, let\u2019s first learn about the identity kernel. The identity kernel is a square matrix, where the middle element is 1, and all other elements are zero, as shown below.</p> \\[K = \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; [1] &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] <p>What makes an identity matrix special is that multiplying it with any other matrix will return the original matrix.</p> <pre><code>img = cv2.imread('ditto_gray.jpeg',0)\n# plot original \nplt.figure(figsize=(10,12))\nplt.subplot(131)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image')\n# ----------- #\n# identity filter \n# ----------- #\nkernel = np.array([[0,0,0],[0,1,0],[0,0,0]], dtype = np.float32)\nddepth = -1\nimg_corr = cv2.filter2D(img, ddepth, kernel, borderType = cv2.BORDER_DEFAULT)\nplt.subplot(132)\nplt.imshow(img_corr, cmap = 'gray')\nplt.axis('off')\nplt.title('Identity filter')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#averaging-filter","title":"Averaging filter","text":"<p>This is done by convolving an image with a normalized box filter. It simply takes the average of all the pixels under the kernel area and replaces the central element. The following are examples of the kernel for implementing an averaging filter: </p> \\[K = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\                                    1 &amp; [1] &amp; 1 \\\\                                   1 &amp; 1 &amp; 1 \\end{bmatrix},\\] \\[K = \\frac{1}{16} \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\                                    2 &amp; [4] &amp; 2 \\\\                                   1 &amp; 2 &amp; 1 \\end{bmatrix},\\] \\[K = \\frac{1}{25} \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\                                     1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\                                     1 &amp; 1 &amp; [1] &amp; 1 &amp; 1\\\\                                     1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\                                     1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix}\\dots\\] <p>In OpenCV you can implement the averaging filter in three ways:</p> <ul> <li>creating a kernel and applying it with the cv2.filter2D function</li> <li>with the cv2.blur function</li> <li>with the cv2.boxFilter function</li> </ul> <pre><code>img = cv2.imread('img/landscape.jpg',1)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# plot original \nplt.figure(figsize=(20,24))\nplt.subplot(131)\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.title('Original image')\n# dimension of kernel \ndim_kernel = 7\n# --------------- #\n# averaging with filter2D function\n# --------------- #\nkernel = np.ones((dim_kernel,dim_kernel), dtype=np.float32)\nkernel = kernel/dim_kernel ** 2\nddepth = -1\nimg_corr = cv2.filter2D(img_rgb, ddepth, kernel, borderType = cv2.BORDER_REFLECT)\nplt.subplot(132)\nplt.imshow(img_corr)\nplt.axis('off')\nplt.title('Averaging with filter2D')\n# --------------- #\n# blur function\n# --------------- #\nimg_blur = cv2.blur(img_rgb,(dim_kernel,dim_kernel))\nplt.subplot(133)\nplt.imshow(img_blur)\nplt.axis('off')\nplt.title('Averaging with blur')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#other-types-of-kernels-for-averaging-blur-filter","title":"Other types of kernels for averaging (blur) filter","text":"<p>In the previous examples, we always used a 2D kernel for applying the averaging filter. But we can construct a \"single row\" kernel to define alternative blur on the image. In particular, we can define kernel for:</p> <ul> <li>Horizontal convolution (correlation)</li> </ul> \\[K = \\frac{1}{5}\\begin{bmatrix} 1 &amp; 1 &amp; [1] &amp; 1 &amp; 1 \\end{bmatrix}\\] <ul> <li>Vertical convolution (correlation)</li> </ul> \\[K = \\frac{1}{5}\\begin{bmatrix} 1 \\\\ 1 \\\\ [1] \\\\ 1 \\\\ 1 \\end{bmatrix}\\] <pre><code>img = cv2.imread('img/landscape.jpg',1)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# plot original \nplt.figure(figsize=(20,24))\nplt.subplot(131)\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.title('Original image')\n# dimension of kernel \ndim_kernel = 7\n# --------------- #\n# Horizontal convolution\n# --------------- #\nimg_blur = cv2.blur(img_rgb,(dim_kernel,1)) # (width and height)\nplt.subplot(132)\nplt.imshow(img_blur)\nplt.axis('off')\nplt.title('Horizontal convolution')\n# --------------- #\n# Vertical convolution\n# --------------- #\nimg_blur = cv2.blur(img_rgb,(1,dim_kernel)) # (width and height)\nplt.subplot(133)\nplt.imshow(img_blur)\nplt.axis('off')\nplt.title('Vertical convolution')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#gaussian-blurring","title":"Gaussian Blurring","text":"<p>In this method, instead of a box filter, a Gaussian kernel is used. It is done with the function, cv2.GaussianBlur().</p> <p><code>cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])</code></p> <p>We should specify the width and height of the kernel (ksize) which should be positive and odd. We also should specify the standard deviation in the X and Y directions, sigmaX and sigmaY respectively. If only sigmaX is specified, sigmaY is taken as the same as sigmaX (the higher number of sigma, it is the higher of smoothness too). If both are given as zeros, they are calculated from the kernel size. Gaussian blurring is highly effective in removing Gaussian noise from an image.</p> <p>Gaussian blurring is commonly used when reducing the size of an image. When downsampling an image, it is common to apply a low-pass filter to the image prior to resampling. This is to ensure that spurious high-frequency information does not appear in the downsampled image. Gaussian blurs have nice properties, such as having no sharp edges, and thus do not introduce ringing into the filtered image.</p> <pre><code>img = cv2.imread('img/landscape.jpg',1)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# plot original \nplt.figure(figsize=(20,24))\nplt.subplot(131)\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.title('Original image')\n# dimension of kernel \ndim_kernel = 7\n# --------------- #\n# Gaussian blurring \n# --------------- #\nimg_gauss = cv2.GaussianBlur(img_rgb, (dim_kernel,dim_kernel), 0)\nplt.subplot(132)\nplt.imshow(img_gauss)\nplt.axis('off')\nplt.title('Gaussian blurring')\n# sigma 10\nimg_gauss = cv2.GaussianBlur(img_rgb, (dim_kernel,dim_kernel), 10)\nplt.subplot(133)\nplt.imshow(img_gauss)\nplt.axis('off')\nplt.title('Gaussian blurring with sigma = 10')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#edge-detectors","title":"Edge Detectors","text":"<p>Detecting edges is one of the fundamental operations you can do in image processing. It helps you reduce the amount of data (pixels) to process and maintains the \"structural\" aspect of the image. We'll look at two  commonly used edge detection schemes:</p> <ul> <li>the gradient based edge detector (sobel)</li> <li>the laplacian based edge detector. </li> </ul> <p>Both of them work with convolutions and achieve the same end goal -&gt; finding edges. All of these methods are based on derivatives, but why may be important the calculus of the derivatives in an image? You can easily notice that in an edge, the pixel intensity changes in a notorious way. A good way to express changes is by using derivatives. A high change in gradient indicates a major change in the image.</p> <p> <p>To be more graphical, let's assume we have a 1D-image. An edge is shown by the \"jump\" in intensity in the plot below:</p> <p> <p>The edge \"jump\" can be seen more easily if we take the first derivative (actually, here appears as a maximum)</p> <p> <p>So, from the explanation above, we can deduce that a method to detect edges in an image can be performed by locating pixel locations where the gradient is higher than its neighbors (or to generalize, higher than a threshold).</p>"},{"location":"LocalOperations/#sobel","title":"Sobel","text":"<p>The Sobel edge detector is a gradient based method. It works with first order derivatives. It calculates the first derivatives of the image separately for the X and Y axes. The derivatives are only approximations (because the images are not continuous).</p>"},{"location":"LocalOperations/#theory-behind-edge-detectors-gradient-based","title":"Theory behind edge detectors gradient based","text":"<ul> <li>Derivative of a continuous function represents the amount of change in the function (as shown in the previous examples). </li> <li>Taking into consideration a 2D function the partial derivaives represents the amount of change along each dimension. </li> </ul> <p> <ul> <li>Gradient (partial derivatives) represents the direction of most rapid change in intensity</li> </ul> \\[\\nabla I =\\biggl[ \\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y} \\biggr]\\] <p> \\(\\nabla I =\\biggl[ \\frac{\\partial I}{\\partial x}, 0 \\biggr]\\) <p> \\(\\nabla I =\\biggl[0, \\frac{\\partial I}{\\partial y} \\biggr]\\) <p> \\(\\nabla I =\\biggl[\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y} \\biggr]\\) <ul> <li> <p>From this two number (\\(\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y}\\)) at each pixels (or function points) you can find both the strength (magnitude) of the edge as well as the orientation of the edge.</p> <ul> <li>Gradient magnitude \\(S = ||\\nabla I|| = \\sqrt{\\bigl(\\frac{\\partial I}{\\partial x}\\bigr)^2 + \\bigl(\\frac{\\partial I}{\\partial y}\\bigr)^2}\\)</li> <li>Gradient orientation \\(\\theta = tan^{-1}\\bigl(\\frac{\\partial I}{\\partial y}/\\frac{\\partial I}{\\partial x}\\bigr)\\)</li> </ul> </li> </ul> <p>That's all done in a continuous domain, how we might implement it and apply it to a discrete image? starting with the finite difference approximations for a trivial image compose by 4 pixels:</p> \\[\\begin{bmatrix} I_{i,j} &amp; I_{i,j+1} \\\\ I_{i+1,j} &amp; I_{i+1,j+1} \\end{bmatrix}\\] <p>In this case we can calculate the partial derivatives as:</p> \\[\\frac{\\partial I}{\\partial x} \\approx \\frac{1}{2\\epsilon}\\biggl(\\bigl(  I_{i,j+1} - I_{i,j}\\bigr) + (I_{i+1,j+1} - I_{i+1,j})\\biggr)\\] \\[\\frac{\\partial I}{\\partial x} \\approx \\frac{1}{2\\epsilon}\\biggl(\\bigl( I_{i,j} - I_{i+1,j} \\bigr) + (I_{i+1,j+1} - I_{i,j+1})\\biggr)\\] <p>where \\(\\epsilon\\) represent the physical distance between the pixels (\\(\\epsilon\\) has only a scaling effect). We can implement the calculation of partial derivatives using the convolution (correlation) operation. </p> <ul> <li>\\(\\frac{\\partial I}{\\partial x} \\approx \\frac{1}{2\\epsilon} \\cdot \\begin{bmatrix} -1 &amp; 1 \\\\ -1  &amp; 1\\end{bmatrix}\\)</li> <li>\\(\\frac{\\partial I}{\\partial y} \\approx \\frac{1}{2\\epsilon} \\cdot\\begin{bmatrix} -1 &amp; -1 \\\\ 1  &amp; 1\\end{bmatrix}\\)</li> </ul> <p>Using this apprach a variaty of gradient operators have been proposed over the last few decades:</p> \\[\\frac{\\partial I}{\\partial x} \\rightarrow \\begin{bmatrix} 0 &amp; 1 \\\\ -1  &amp; 0\\end{bmatrix},\\;\\;\\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1\\end{bmatrix},\\;\\; \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1\\end{bmatrix},\\;\\; \\begin{bmatrix} -1 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ -2 &amp; -3 &amp; 0 &amp; 3 &amp; 2 \\\\ -3 &amp; -5 &amp; 0 &amp; 5 &amp; 3 \\\\ -2 &amp; -3 &amp; 0 &amp; 3 &amp; 2 \\\\ -1 &amp; -2 &amp; 0 &amp; 2 &amp; 1\\end{bmatrix}\\] \\[\\frac{\\partial I}{\\partial y} \\rightarrow \\begin{bmatrix} -1 &amp; 0 \\\\ 0  &amp; 1\\end{bmatrix},\\;\\;\\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1\\end{bmatrix},\\;\\; \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1\\end{bmatrix},\\;\\; \\begin{bmatrix} -1 &amp; -2 &amp; -3 &amp; -2 &amp; -1 \\\\ -2 &amp; -3 &amp; -5 &amp; -3 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 2 &amp; 3 &amp; 5 &amp; 3 &amp; 2 \\\\ 1 &amp; 2 &amp; 3 &amp; 2 &amp; 1\\end{bmatrix}\\] <p>Respectively: Roberts, Prewitt, Sobel (3x3), Sobel (5x5).</p> <ul> <li>Small filter: <ul> <li>good localization</li> <li>noise sensitive</li> <li>poor detection</li> </ul> </li> <li>Larger fiter:<ul> <li>poor localization</li> <li>less noise sensitive</li> <li>good detection</li> </ul> </li> </ul>"},{"location":"LocalOperations/#example-of-sobel-operation","title":"Example of Sobel operation","text":"<p>Remember to apply the Sobel operator on grayscale images </p> <pre><code>img = cv2.imread('img/sudoku.png',0)\n# plot original \nplt.figure(figsize=(10,12))\nplt.subplot(221)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image')\n# sobel x \nimg_sobelx = cv2.Sobel(img, ddepth=-1, dx=1, dy=0, ksize=3)\nplt.subplot(222)\nplt.imshow(img_sobelx, cmap = 'gray')\nplt.axis('off')\nplt.title('Sobel x')\n# sobel y \nimg_sobely = cv2.Sobel(img, ddepth=-1, dx=0, dy=1, ksize=3)\nplt.subplot(223)\nplt.imshow(img_sobely, cmap = 'gray')\nplt.axis('off')\nplt.title('Sobel y')\n# sobel xy\nimg_sobelxy = cv2.Sobel(img, ddepth=-1, dx=1, dy=1, ksize=3)\nplt.subplot(224)\nplt.imshow(img_sobelxy, cmap = 'gray')\nplt.axis('off')\nplt.title('Sobel xy')\nplt.show()\n</code></pre> <p> <pre><code># sobel operator on complex image \nimg = cv2.imread('img/lenna.png',0)\n# plot original \nplt.figure(figsize=(12,14))\nplt.subplot(221)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image')\n# sobel \nimg_sobelxy = cv2.Sobel(img, ddepth=-1, dx=1, dy=1, ksize=7)\nplt.subplot(222)\nplt.imshow(img_sobelxy, cmap = 'gray')\nplt.axis('off')\nplt.title('Sobel xy')\n# gaussian blurring\ndim_kernel = 7\nimg_gauss = cv2.GaussianBlur(img, (dim_kernel,dim_kernel), 10)\nplt.subplot(223)\nplt.imshow(img_gauss, cmap = 'gray')\nplt.axis('off')\nplt.title('Gaussian blurring with sigma = 10')\n# sobel \nimg_sobelxy = cv2.Sobel(img_gauss, ddepth=-1, dx=1, dy=1, ksize=7)\nplt.subplot(224)\nplt.imshow(img_sobelxy, cmap = 'gray')\nplt.axis('off')\nplt.title('Sobel xy blurred image')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#laplace-filter","title":"Laplace filter","text":"<p>Sobel Operator is based on the fact that in the edge area, the pixel intensity shows a \"jump\" or a high variation of intensity. Getting the first derivative of the intensity, we observed that an edge is characterized by a maximum. What happens if we take the second derivative?</p> <p> <p>You can observe that the second derivative is zero so, we can also use this criterion to attempt to detect edges in an image (at the edges you get very strong zero-crossing). However, note that zeros will not only appear in edges (they can actually appear in other meaningless locations); this can be solved by applying filtering where needed (threshold the zero-crossing in terms of how rapid it is).</p>"},{"location":"LocalOperations/#theory-behind-the-laplace-operator","title":"Theory behind the laplace operator","text":"<ul> <li>Laplacian: sum of pure second derivatives </li> </ul> \\[\\nabla^2I=\\frac{\\partial^2 I}{\\partial x}+\\frac{\\partial^2 I}{\\partial y}\\] <ul> <li>Edges are \"zero-crossing\" in Laplacian of image </li> <li>Laplacian does not provide directions of edges </li> </ul> <p>For the explanation of the discrete Laplacian operator, we need a 3x3 grid of pixels (because the second derivative is in terms of finite differences the difference of the differencem, so we need at least 3 pixels). </p> \\[\\begin{bmatrix} I_{i-1,j-1} &amp; I_{i-1,j} &amp; I_{i-1,j+1} \\\\ I_{i,j-1} &amp; I_{i,j} &amp; I_{i,j+1} \\\\ I_{i+1,j-1} &amp; I_{i+1,j} &amp; I_{i+1,j+1}\\end{bmatrix}\\] <p>with this simple matrix, let's say we want to output of the Laplacian operator for the central pixel \\(I_{i,j}\\)</p> \\[\\frac{\\partial^2 I}{\\partial x} \\approx \\frac{1}{\\epsilon}\\biggl(\\frac{1}{\\epsilon}\\bigl(I_{i,j+1}-I_{i,j}\\bigr)-\\frac{1}{\\epsilon}\\bigl(I_{i,j}-I_{i,j-1}\\bigr)   \\biggr) \\approx \\frac{1}{\\epsilon^2}\\bigl(I_{i,j-1}-2I_{i,j}+I_{i,j+1}\\bigr)\\] \\[\\frac{\\partial^2 I}{\\partial y} \\approx \\frac{1}{\\epsilon}\\biggl(\\frac{1}{\\epsilon}\\bigl(I_{i+1,j}-I_{i,j}\\bigr)-\\frac{1}{\\epsilon}\\bigl(I_{i,j}-I_{i-1,j}\\bigr)   \\biggr) \\approx \\frac{1}{\\epsilon^2}\\bigl(I_{i-1,j}-2I_{i,j}+I_{i+1,j}\\bigr)\\] \\[\\nabla^2I=\\frac{\\partial^2 I}{\\partial x}+\\frac{\\partial^2 I}{\\partial y}\\] <p>Finally the operator can be implemented in terms of convolution with the following kernel form:</p> \\[\\nabla^2 \\approx \\frac{1}{\\epsilon^2} \\begin{bmatrix}0&amp;1&amp;0 \\\\ 1&amp;-4&amp;1 \\\\ 0&amp;1&amp;0 \\end{bmatrix}\\] <p>the corner values of the kernel is equal to 0 because we nerver end up using the corner pixels of the original image 3x3. </p> <ul> <li>However, there is a slight problem with this form of the kernel, we know that edges can appear in any orientation and let's say they, an edge appeared at 45 degrees, in that case we see that we haven't taken into account the distance in this diagonal direction (we have 0 at the corner of the kernel). Since we are on a discrete grid, you can modify the original filter and obtaining a filter that looks like:</li> </ul> \\[\\nabla^2 \\approx \\frac{1}{6\\epsilon^2} \\begin{bmatrix}1&amp;4&amp;1 \\\\ 4&amp;-20&amp;4 \\\\ 1&amp;4&amp;1 \\end{bmatrix}\\]"},{"location":"LocalOperations/#example-of-laplacian-operator","title":"Example of Laplacian operator","text":"<pre><code># sobel operator on complex image \nimg = cv2.imread('img/lenna.png',0)\n# plot original \nplt.figure(figsize=(12,14))\nplt.subplot(221)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image')\n# laplacian on original  \ndim_kernel = 3\nimg_lapc = cv2.Laplacian(img,ddepth,(dim_kernel,dim_kernel))\nplt.subplot(222)\nplt.imshow(img_lapc, cmap = 'gray')\nplt.axis('off')\nplt.title('Laplacian operator on original image')\n# gaussian blurring\ndim_kernel = 7\nimg_gauss = cv2.GaussianBlur(img, (dim_kernel,dim_kernel), 10)\nplt.subplot(223)\nplt.imshow(img_gauss, cmap = 'gray')\nplt.axis('off')\nplt.title('Gaussian blurring with sigma = 10')\n# sobel \ndim_kernel = 3\nimg_lapc = cv2.Laplacian(img_gauss,ddepth,(dim_kernel,dim_kernel))\nplt.subplot(224)\nplt.imshow(img_lapc, cmap = 'gray')\nplt.axis('off')\nplt.title('Laplacian operator on blurred image')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#gradient-vs-laplacian","title":"Gradient vs Laplacian","text":"<p>"},{"location":"LocalOperations/#non-lineal-filters","title":"Non-lineal filters","text":"<p>The idea is to replace the target pixel value with its neighbor pixels value from some ordering mechanism or function. There are many types of non-linear filters, but the main ones are the following:</p> <ul> <li>Minimum Filter: select the lowest pixel value from the neighbors' pixels around the target then replace it.</li> <li>Maximum Filter: similar to minimum filter but pick the highest one.</li> <li>Median Filter: selects the neighbor pixels then sort it from the lowest to highest then pick the median one to replace at the targeted pixel.</li> </ul>"},{"location":"LocalOperations/#median-filter","title":"Median Filter","text":"<p>OpenCV has the function for the median filter which is the medianBlur function. The function smoothes an image using the median filter with the \\(ksize\\times ksize\\) aperture. Each channel of a multi-channel image is processed independently. In-place operation is supported.</p> <p><code>cv2.medianBlur (src, dst, ksize)</code></p> <ul> <li>src - input image</li> <li>dst - destination array of the same size and type as src.</li> <li>ksize - aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...</li> </ul> <pre><code>img = cv2.imread('img/landscape.jpg',1)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n# plot original \nplt.figure(figsize=(16,18))\nplt.subplot(121)\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.title('Original image')\n# --------------- #\n# median blurring\n# --------------- #\nimg_median = cv2.medianBlur(img_rgb, 7)\nplt.subplot(122)\nplt.imshow(img_median)\nplt.axis('off')\nplt.title('Median blurring')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#image-restoration-example","title":"Image restoration example","text":"<pre><code>img = cv2.imread('img/corrupted.png',0)\n# plot original \nplt.figure(figsize=(16,18))\nplt.subplot(121)\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.title('Original image: corrupted by salt and pepper noise')\n# --------------- #\n# median blurring (restoration)\n# --------------- #\nimg_median = cv2.medianBlur(img, 7)\nplt.subplot(122)\nplt.imshow(img_median, cmap = 'gray')\nplt.axis('off')\nplt.title('Median blurring')\nplt.show()\n</code></pre> <p>"},{"location":"LocalOperations/#references","title":"References","text":"<ul> <li>Computer Vision Course by Prof. Ghidoni Stefano (Unipd)</li> <li>Image Filters with Python and OpenCV</li> <li>Computer Vision: Algorithms and Applications by Richard Szeliski</li> <li>Making your own linear filters</li> <li>Correlation vs Convolution Filtering</li> <li>Python OpenCV \u2013 Filter2D() Function</li> <li>Scipy convolve</li> <li>Smoothing Images</li> <li>Smoothing Images 2</li> <li>Python OpenCV | cv2.blur() method</li> <li>Gaussian Blur</li> <li>Laplacian</li> <li>Sobel and Laplacian</li> <li>Sobel</li> </ul>"},{"location":"SinglePixelOperations/","title":"Single pixel operations","text":""},{"location":"SinglePixelOperations/#single-pixel-operations_1","title":"Single pixel operations","text":"<p>Considering a grayscale image, single pixel operations/transforms (AKA intensity transforms) are functions that change the gray levels (values of pixels from 0 to 255) of the image. Elements involved:</p> <ul> <li>Function \\(I(x,y)\\) representing the original image \\((r=I(x_i,y_i)\\) is the \\(i\\)-th pixel)</li> <li>Function \\(T(\\cdot)\\) representing the gray level change (\\(s=T(r)\\) output level of \\(i\\)-th pixel)</li> </ul> <p>The following are commonly used intensity transformations:</p> <ul> <li>Negative</li> <li>Logarithm</li> <li>Gamma</li> <li>Histogram equalization</li> </ul> <p> <p></p> <pre><code># Libraries \nimport cv2 \nimport numpy as np \nimport matplotlib.pyplot as plt \n%matplotlib inline\n</code></pre> <pre><code>img = cv2.imread('ditto_gray.jpeg',0)\nprint(f'Image dimenions: {img.shape}')\nplt.imshow(img, cmap = 'gray')\nplt.axis('off')\nplt.show()\n</code></pre> <pre><code>Image dimenions: (432, 512)\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#negative-transformations","title":"Negative transformations","text":"<p>Switch dark and light. The negative transformation can be described by the expression</p> \\[s = (L-1)-r, \\;\\; L = 256\\] <p>where \\(r\\) is the initial intensity level and \\(s\\) is the final intensity level of a pixel.</p> <pre><code># apply the negative transformation\nL = 256\nneg_img = (L-1)-img\nplt.imshow(neg_img, cmap = 'gray')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#log-transform","title":"Log transform","text":"<p>Practically, log transformation maps a narrow range of low-intensity input values to a wide range of output values. Mathematically, log transformations can be expressed as</p> \\[s=c\\cdot log(1+r),\\;\\;c=\\frac{L-1}{logL}\\] <p>where \\(c\\) is a scaling constant. The scaling operation is done to ensure that the final pixel value does not exceed (L-1), or 255.</p> <pre><code>fig, ax = plt.subplots()\nx = list(range(1,255))\ny1 = x\ny2 = np.log(x)\n# all possible value of original image [0,255]\nax.plot(x, y1, color=\"blue\")\nax.set_xlabel('r')\nax.set_ylabel('s', color=\"blue\")       \nax.tick_params(axis='y', colors=\"blue\")  \n# log transformation \nax2 = ax.twinx()\nax2.plot(x, y2, color=\"red\") \nplt.axis('off')\n# show \nplt.show()\n</code></pre> <p> <p></p> <pre><code># original image\nimg = cv2.imread('img/log.png',0)\nplt.subplot(121)\nplt.imshow(img, cmap = 'gray')\nplt.title('Original image')\nplt.axis('off')\n# Apply log transform.\nL = 256\nc = (L-1)/(np.log(L))\nimg_log = c * np.log(1 + img)\nplt.subplot(122)\nplt.imshow(img_log, cmap = 'gray')\nplt.title('Log transformed')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#gamma-transform","title":"Gamma transform","text":"<p>Gamma correction is important for displaying images on a screen correctly, to prevent bleaching or darkening of images when viewed from different types of monitors with different display settings. This is done because our eyes perceive images in a gamma-shaped curve, whereas cameras capture images in a linear fashion.</p> <p> <p></p> <p>Gamma transformations can be mathematically expressed as:</p> \\[s = cr^\\gamma \\] \\[c = (L-1)^{1-\\gamma} \\] <p> <p></p> <pre><code># original image \nimg = cv2.imread('ditto_gray.jpeg',0)\n# gamma transformation \nL = 256\ngamma = 0.10\nc = (L-1) ** (1-gamma)\nimg_gamma = c * (img ** gamma)\n# plot\nplt.subplot(121)\nplt.imshow(img, cmap = 'gray')\nplt.title('Original image')\nplt.axis('off')\nplt.subplot(122)\nplt.imshow(img_gamma, cmap = 'gray')\nplt.title('Gamma transformed')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#histogram-equalization","title":"Histogram Equalization","text":"<p>What is an Image Histogram?</p> <ul> <li>It is a graphical representation of the intensity distribution of an image</li> <li>It quantifies the number of pixels for each intensity value considered.</li> </ul> <p> <p></p>"},{"location":"SinglePixelOperations/#histogram-of-a-grayscale-image","title":"Histogram of a grayscale image","text":"<p>OpenCV provides us with the cv2.calcHist function to calculate the image histograms. Let's familiarize with the function and its parameters:</p> <p><code>cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])</code> </p> <ul> <li>images: it is the source image of type uint8 or float32. it should be given in square brackets, ie, \"[img]\".</li> <li>channels: it is also given in square brackets. It is the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0], [1] or [2] to calculate histogram of blue, green or red channel respectively.</li> <li>mask: mask image. To find histogram of full image, it is given as \"None\". But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)</li> <li>histSize: this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].</li> <li>ranges: this is our RANGE. Normally, it is [0,256].</li> </ul> <pre><code># original image \nimg = cv2.imread('img/landscape.jpg',0)\nhist = cv2.calcHist([img],[0],None,[256],[0,256])\nplt.figure(figsize=(9,4))\nplt.subplot(121)\nplt.imshow(img, cmap = 'gray')\nplt.title('Original image')\nplt.subplot(122)\nplt.plot(hist, color='black')\nplt.title('Image Histogram')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#histogram-calculation-in-numpy","title":"Histogram Calculation in Numpy","text":"<pre><code># original image \nimg = cv2.imread('img/landscape.jpg',0)\n# using numpy ravel \nplt.figure(figsize=(9,4))\nplt.subplot(121)\nplt.imshow(img, cmap = 'gray')\nplt.title('Original image')\nplt.subplot(122)\nplt.hist(img.ravel(),256,[0,256])\nplt.title('Image Histogram')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#application-of-mask","title":"Application of Mask","text":"<pre><code>img = cv2.imread('img/landscape.jpg',0)\n# create a mask\nmask = np.zeros(img.shape[:2], np.uint8)\nmask[100:500, 100:600] = 255\nmasked_img = cv2.bitwise_and(img,img,mask = mask)\n# Calculate histogram with mask and without mask\nhist_full = cv2.calcHist([img],[0],None,[256],[0,256])\nhist_mask = cv2.calcHist([img],[0],mask,[256],[0,256])\n# plot\nplt.figure(figsize=(12,8))\nplt.subplot(221), plt.imshow(img, 'gray')\nplt.subplot(222), plt.imshow(mask,'gray')\nplt.subplot(223), plt.imshow(masked_img, 'gray')\nplt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)\nplt.xlim([0,256])\nplt.show()\n</code></pre> <p> <p></p> <p>In the histogram plot, blue line shows histogram of full image while orange line shows histogram of masked region.</p>"},{"location":"SinglePixelOperations/#what-is-histogram-equalization","title":"What is Histogram Equalization?","text":"<ul> <li>It is a method that improves the contrast in an image, in order to stretch out the intensity range</li> <li>Used for: <ul> <li>Evaluating image statistics</li> <li>Compression</li> <li>Segmentation</li> <li>Image enhancement</li> </ul> </li> </ul> <p>Histogram of the grayscale image can be treated as a probabilistic density function (PDF):</p> \\[p(r_k) = \\frac{n_k}{MN}\\]"},{"location":"SinglePixelOperations/#pdfcdf-recall","title":"PDF/CDF recall","text":"<ul> <li> <p>Cumulative Distribution Function (CDF)</p> \\[ F_x(x) = P(X \\leq x)\\] </li> <li> <p>Probability Density Function (PDF)</p> \\[f_x(x) = \\frac{d}{d_x}F_x(x)\\] \\[F_x(x) = \\int_{-\\infty}^{x} f_x(t) dt \\] </li> </ul>"},{"location":"SinglePixelOperations/#equalization-function","title":"Equalization function","text":"<p>Can we modify the histogram? We can equalize the histogram (flattens the histogram) based on an equalization function.</p> <ul> <li>Introduce \\(T(r)\\) to equalize the histogram</li> <li>\\(T(r)\\) shall be monotonically non-decreasing (or monotonically increasing if inverse function is needed)</li> </ul> \\[ 0 \\leq T(r) \\leq L-1 \\] \\[ 0 \\leq r \\leq L-1\\] <p>where \\(T(r)\\) continuous and differentiable. The CDF of the \\(p(r)\\) is the \\(T(r)\\) transformation used to equalize the histogram:</p> \\[ s = T(r) = (L-1)\\int_{-\\infty}^r p_r(w)dw,\\;\\;L=256 \\] \\[ s_k = T(r_k) = (L-1)\\sum_{j=0}^k p_r(r_j)\\] <p> Equalization: a 3-bit example <p></p> <p>The output is not perfectly flat caused by the discrete nature of data. Finally, we use a simple remapping procedure to obtain the intensity values of the equalized image:</p> \\[equilized(x_i,y_i) = T((x_i,y_i))\\] <p>Following the previous example in a hypothetical 3-bit image:</p> <ul> <li>all point \\((x_i,y_i)\\) with value 0 is mapped to value 1</li> <li>all point \\((x_i,y_i)\\) with value 1 is mapped to value 3</li> <li>all point \\((x_i,y_i)\\) with value 2 is mapped to value 5</li> <li>\\(\\dots\\)</li> </ul> <pre><code># original image \nimg = cv2.imread('img/landscape.jpg',0)\nhist = cv2.calcHist([img],[0],None,[256],[0,256])\nplt.figure(figsize=(10,8))\nplt.subplot(221)\nplt.imshow(img, cmap = 'gray')\nplt.title('Original image')\nplt.subplot(222)\nplt.hist(img.ravel(),256,[0,256])\nplt.title('Image histogram')\n# equalize \nimg_equalized = cv2.equalizeHist(img)\nplt.subplot(223)\nplt.imshow(img_equalized, cmap = 'gray')\nplt.title('Equalized image')\nplt.subplot(224)\nplt.hist(img_equalized.ravel(),256,[0,256])\nplt.title('Image histogram')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#histogram-equalization-on-rgb-image","title":"Histogram equalization on RGB image","text":"<p>For a greyscale image, each pixel is represented by the intensity value (brightness); that is why we can feed the pixel values directly to the HE (histogram equalization) function. However, that is not how it works for an RGB-formatted color image. Each channel of the R, G, and B represents the intensity of the related color, not the intensity/brightness of the image as a whole. And so, running HE on these color channels is not the proper way.</p> <p>We should first separate the brightness of the image from the color and then run HE on the brightness. Now, there are already standardized colorspaces that encode brightness and color separately, like YCbCr (Y is the luma component of the color. Luma component is the brightness of the color. Cb and Cr is the blue component and red component related to the chroma component. That means \u201cCb is the blue component relative to the green component. Cr is the red component relative to the green component), HSV, \\(\\dots\\)so, we can use them for separating and then re-merging the brightness. The proper way:</p> <ul> <li>Convert the colorspace from RGB to YCbCr;</li> <li>Run HE on the Y channel (this channel represents brightness);</li> <li>Convert back the colorspace to RGB.</li> </ul> <pre><code># original image \nimg = cv2.imread('img/landscape.jpg',1)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,8))\nplt.subplot(121)\nplt.imshow(img_rgb)\nplt.title('Original image')\nplt.axis('off')\n# convert from RGB color-space to YCrCb\nimg_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n# equalize the histogram of the Y channel\nimg_ycrcb[:, :, 0] = cv2.equalizeHist(img_ycrcb[:, :, 0])\n# convert back to RGB color-space from YCrCb\nimg_equalized = cv2.cvtColor(img_ycrcb, cv2.COLOR_YCrCb2RGB)\nplt.subplot(122)\nplt.imshow(img_equalized)\nplt.title('Equalized image')\nplt.axis('off')\nplt.show()\n</code></pre> <p> <p></p>"},{"location":"SinglePixelOperations/#references","title":"References","text":"<ul> <li>Computer Vision Course by Prof. Ghidoni Stefano (Unipd)</li> <li>OpeCV Histograms</li> <li>OpenCV Histogram Equalization </li> </ul>"}]}